{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_finetune_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Fine tune a Bert model for a QA task"
      ],
      "metadata": {
        "id": "nsJWMeWsELRn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.1 Preparation\n",
        "We start by bringing in the required components: We install the transformers library to use its tools, and then from it we import a tokenizer, and, more importantly, the DistilBert moodel for question answering and Adamw, an optimizer with a fixed weight decay."
      ],
      "metadata": {
        "id": "cvJLlTgOC9yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "# Import Libraries\n",
        "import torch\n",
        "from transformers import AutoTokenizer, DistilBertForQuestionAnswering, AdamW\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt \n",
        "import os\n",
        "from fastai.imports import *"
      ],
      "metadata": {
        "id": "GyRVxxjyoPUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c43d3d-9ccd-4f72-e60f-39457bdb4ebe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 12.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 31.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we import our data, which are the 1.1 version validation set of the squad dataset. Our tokenizer is also defined to a specific model."
      ],
      "metadata": {
        "id": "yI4ZrXW1EZ9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir squad\n",
        "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json -O squad/dev-v1.1.json\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "-ld-mrVRpdbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we bring our data in lists' form so they can be used by our model. The total length of our dataset is 34,726 context/question/answer triplets. From those, we use the first 33,000 as our train data, and the last 1,726 as our validation data. We also print out the first triplet to check that they're in the correct form. "
      ],
      "metadata": {
        "id": "rGjOSxhUE5_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Give the path for train data\n",
        "path = Path('squad/dev-v1.1.json')\n",
        "\n",
        "# Open .json file\n",
        "with open(path, 'rb') as f:\n",
        "    squad_dict = json.load(f)\n",
        "\n",
        "texts = []\n",
        "queries = []\n",
        "answers = []\n",
        "\n",
        "# Search for each passage, its question and its answer\n",
        "for group in squad_dict['data']:\n",
        "    for passage in group['paragraphs']:\n",
        "        context = passage['context']\n",
        "        for qa in passage['qas']:\n",
        "            question = qa['question']\n",
        "            for answer in qa['answers']:\n",
        "                # Store every passage, query and its answer to the lists\n",
        "                texts.append(context)\n",
        "                queries.append(question)\n",
        "                answers.append(answer)\n",
        "\n",
        "print('Total train set length:', len(texts))\n",
        "train_texts, train_queries, train_answers = texts[:33000], queries[:33000], answers[:33000]\n",
        "val_texts,   val_queries,   val_answers   = texts[33000:], queries[33000:], answers[33000:]\n",
        "print()\n",
        "\n",
        "print(\"Passage: \",train_texts[0])  \n",
        "print(\"Query: \",train_queries[0])\n",
        "print(\"Answer: \",train_answers[0])\n",
        "\n",
        "# train_texts   = train_texts[:10]\n",
        "# train_queries = train_queries[:10]\n",
        "# train_answers = train_answers[:10] "
      ],
      "metadata": {
        "id": "ZI1xuOPHoPkO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd39c867-a897-4015-f73a-e2965b575c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train set length: 34726\n",
            "\n",
            "Passage:  Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
            "Query:  Which NFL team represented the AFC at Super Bowl 50?\n",
            "Answer:  {'answer_start': 177, 'text': 'Denver Broncos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Bert model needs the start and end positions of the answers to work, so here these are specified for both train and validation sets. For the end position, a minor tweak has been made, because the squad dataset tends to be up to 2 characters short of the real answer."
      ],
      "metadata": {
        "id": "jqZg8MZnH3ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find start and end positions for train set\n",
        "\n",
        "for answer, text in zip(train_answers, train_texts):\n",
        "    real_answer = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    # Get the real end index\n",
        "    end_idx = start_idx + len(real_answer)\n",
        "\n",
        "    # Deal with the problem of 1 or 2 more characters \n",
        "    if text[start_idx:end_idx] == real_answer:\n",
        "        answer['answer_end'] = end_idx\n",
        "    # When the real answer is more by one character\n",
        "    elif text[start_idx-1:end_idx-1] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 1\n",
        "        answer['answer_end'] = end_idx - 1  \n",
        "    # When the real answer is more by two characters  \n",
        "    elif text[start_idx-2:end_idx-2] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 2\n",
        "        answer['answer_end'] = end_idx - 2    \n",
        "\n",
        "# Find start and end positions for validation set\n",
        "\n",
        "for answer, text in zip(val_answers, val_texts):\n",
        "    real_answer = answer['text']\n",
        "    start_idx = answer['answer_start']\n",
        "    # Get the real end index\n",
        "    end_idx = start_idx + len(real_answer)\n",
        "\n",
        "    # Deal with the problem of 1 or 2 more characters \n",
        "    if text[start_idx:end_idx] == real_answer:\n",
        "        answer['answer_end'] = end_idx\n",
        "    # When the real answer is more by one character\n",
        "    elif text[start_idx-1:end_idx-1] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 1\n",
        "        answer['answer_end'] = end_idx - 1  \n",
        "    # When the real answer is more by two characters  \n",
        "    elif text[start_idx-2:end_idx-2] == real_answer:\n",
        "        answer['answer_start'] = start_idx - 2\n",
        "        answer['answer_end'] = end_idx - 2  "
      ],
      "metadata": {
        "id": "-ilQl9cWqv6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we tokenize our data."
      ],
      "metadata": {
        "id": "g58FvEASLX9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\n",
        "val_encodings   = tokenizer(val_texts,   val_queries,   truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "GiguMywLqv_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next function is used to insert start-end tokens to the respective positions. "
      ],
      "metadata": {
        "id": "m6JTXx0jIbv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_token_positions(encodings, answers):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
        "        # if None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = tokenizer.model_max_length\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "add_token_positions(train_encodings, train_answers)\n",
        "add_token_positions(val_encodings,     val_answers)"
      ],
      "metadata": {
        "id": "F54bvlkFIc4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three last things required to lay the groundwork for finetuning our model: One is to create a class for the train and val datasets, to facilitate training and to convert encodings to datasets. The second is to define a Dataloader, to input the data in a shuffled batch (size 8). Lastly, we define the type of device used (cuda or CPU) and print which one it is."
      ],
      "metadata": {
        "id": "f5i94wSGMVOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and val dataset classes\n",
        "class SquadDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = SquadDataset(train_encodings)\n",
        "val_dataset = SquadDataset(val_encodings)\n",
        "\n",
        "# Define Dataloader\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device is', device)"
      ],
      "metadata": {
        "id": "GQ5ZW0TYqwFv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9030c55d-50cc-40df-8f5c-0bf6cb05f5c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1.2 Finetuning the model\n",
        "We load the model required for this task (DistilBert For Question Answering). We also load AdamW as our optimizer, and we define our train epochs to 4."
      ],
      "metadata": {
        "id": "vTuN7nX-PTvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "optim = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "epochs = 4 "
      ],
      "metadata": {
        "id": "McEgZ5iOrd4r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f5f6b4-ee23-47fc-ec01-7fb50bf0f1e0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train the model"
      ],
      "metadata": {
        "id": "FqwzEgcrV2Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whole_train_eval_time = time.time()\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "model.to(device)\n",
        "print_every = 1000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_time = time.time()\n",
        "\n",
        "  model.train() # Set model in train mode\n",
        "    \n",
        "  loss_of_epoch = 0\n",
        "\n",
        "  print(\"TRAIN\")\n",
        "\n",
        "  for batch_idx,batch in enumerate(train_loader): \n",
        "    \n",
        "    optim.zero_grad()\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    start_positions = batch['start_positions'].to(device)\n",
        "    end_positions = batch['end_positions'].to(device)\n",
        "    \n",
        "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "    loss = outputs[0]\n",
        "    # do a backwards pass \n",
        "    loss.backward()\n",
        "    # update the weights\n",
        "    optim.step()\n",
        "    # Find the total loss\n",
        "    loss_of_epoch += loss.item()\n",
        "\n",
        "    if (batch_idx+1) % print_every == 0:\n",
        "      print(\"Batch {:} / {:}\".format(batch_idx+1,len(train_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
        "\n",
        "  loss_of_epoch /= len(train_loader)\n",
        "  train_losses.append(loss_of_epoch)\n",
        "\n",
        "  # Evaluation\n",
        "\n",
        "  # Set model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  print(\"EVALUATE\")\n",
        "\n",
        "  loss_of_epoch = 0\n",
        "\n",
        "  for batch_idx,batch in enumerate(val_loader):\n",
        "    \n",
        "    with torch.no_grad():\n",
        "\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      start_positions = batch['start_positions'].to(device)\n",
        "      end_positions = batch['end_positions'].to(device)\n",
        "      \n",
        "      outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "      loss = outputs[0]\n",
        "      # Find the total loss\n",
        "      loss_of_epoch += loss.item()\n",
        "\n",
        "    if (batch_idx+1) % print_every == 0:\n",
        "       print(\"Batch {:} / {:}\".format(batch_idx+1,len(val_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n",
        "\n",
        "  loss_of_epoch /= len(val_loader)\n",
        "  val_losses.append(loss_of_epoch)\n",
        "\n",
        "  # Print each epoch's time and train/val loss \n",
        "  print(\"\\nEpoch \", epoch+1,\n",
        "        \"\\nTraining Loss:\", train_losses[-1],\n",
        "        \"\\nValidation Loss:\", val_losses[-1],\n",
        "        \"\\nTime: \",(time.time() - epoch_time),\n",
        "        \"\\n\\n\")\n",
        "\n",
        "print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89oViPGloPz9",
        "outputId": "07a8d7e6-5c92-4165-bd75-99dccb33f74a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN\n",
            "Batch 1000 / 4125 \n",
            "Loss: 1.4 \n",
            "\n",
            "Batch 2000 / 4125 \n",
            "Loss: 1.4 \n",
            "\n",
            "Batch 3000 / 4125 \n",
            "Loss: 2.9 \n",
            "\n",
            "Batch 4000 / 4125 \n",
            "Loss: 0.6 \n",
            "\n",
            "EVALUATE\n",
            "\n",
            "Epoch  1 \n",
            "Training Loss: 1.5312047022617223 \n",
            "Validation Loss: 1.8528673295621518 \n",
            "Time:  3351.294318675995 \n",
            "\n",
            "\n",
            "TRAIN\n",
            "Batch 1000 / 4125 \n",
            "Loss: 1.3 \n",
            "\n",
            "Batch 2000 / 4125 \n",
            "Loss: 0.7 \n",
            "\n",
            "Batch 3000 / 4125 \n",
            "Loss: 1.0 \n",
            "\n",
            "Batch 4000 / 4125 \n",
            "Loss: 0.6 \n",
            "\n",
            "EVALUATE\n",
            "\n",
            "Epoch  2 \n",
            "Training Loss: 0.8931168509685632 \n",
            "Validation Loss: 1.8838744224221617 \n",
            "Time:  3348.6583893299103 \n",
            "\n",
            "\n",
            "TRAIN\n",
            "Batch 1000 / 4125 \n",
            "Loss: 0.2 \n",
            "\n",
            "Batch 2000 / 4125 \n",
            "Loss: 0.7 \n",
            "\n",
            "Batch 3000 / 4125 \n",
            "Loss: 0.6 \n",
            "\n",
            "Batch 4000 / 4125 \n",
            "Loss: 0.7 \n",
            "\n",
            "EVALUATE\n",
            "\n",
            "Epoch  3 \n",
            "Training Loss: 0.7490737383230166 \n",
            "Validation Loss: 1.952481464655311 \n",
            "Time:  3347.89563536644 \n",
            "\n",
            "\n",
            "TRAIN\n",
            "Batch 1000 / 4125 \n",
            "Loss: 0.3 \n",
            "\n",
            "Batch 2000 / 4125 \n",
            "Loss: 1.4 \n",
            "\n",
            "Batch 3000 / 4125 \n",
            "Loss: 1.0 \n",
            "\n",
            "Batch 4000 / 4125 \n",
            "Loss: 0.3 \n",
            "\n",
            "EVALUATE\n",
            "\n",
            "Epoch  4 \n",
            "Training Loss: 0.6651255462043213 \n",
            "Validation Loss: 2.0054682562196695 \n",
            "Time:  3332.7469170093536 \n",
            "\n",
            "\n",
            "Total training and evaluation time:  13380.674303293228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig,ax = plt.subplots(1,1,figsize=(15,10))\n",
        "\n",
        "ax.set_title(\"Train and Validation Losses\",size=20)\n",
        "ax.set_ylabel('Loss', fontsize = 20) \n",
        "ax.set_xlabel('Epochs', fontsize = 25) \n",
        "_=ax.plot(train_losses)\n",
        "_=ax.plot(val_losses)\n",
        "_=ax.legend(('Train','Val'),loc='upper right')"
      ],
      "metadata": {
        "id": "yT6UgeBXrtCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we save the finetuned model and the tokenizer used, to be used in the evaluation stage."
      ],
      "metadata": {
        "id": "shb_syU8tgd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoke google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Save model and tokenizer\n",
        "torch.save(model,\"/content/drive/MyDrive/NLU/model220130/model.bin\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/NLU/model220130\")\n"
      ],
      "metadata": {
        "id": "9tEFseBFgjg_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}